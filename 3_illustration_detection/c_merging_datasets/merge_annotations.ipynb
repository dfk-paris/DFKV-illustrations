{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6eaa52",
   "metadata": {},
   "source": [
    "# Merging datasets annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52dcb21",
   "metadata": {},
   "source": [
    "We first want to take from the newspaper dataset only the images that could also be used to train the model with our goal in mind. That means, we only keep newspapers that have at least one of the following label : Photograph (0) or Comics/cartoon (3). We then rename all these annotations as just 'Illustration', to stay in line with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "784bb6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ff18e",
   "metadata": {},
   "source": [
    "We start by loading the training data from the newspaper navigator project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398f95d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load newspaper annotations\n",
    "f = open('data/trainval.json')\n",
    "data = json.load(f)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb89b31",
   "metadata": {},
   "source": [
    "We first need to adapt their data to fit our needs : only keep the approriate images and change their labels to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a43ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change the label to 1\n",
    "def replace_label(x):\n",
    "    try: \n",
    "        x['category_id'] = 0\n",
    "        return x\n",
    "    except:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "057d5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep annotations with labels 0 or 3\n",
    "new_annotations = [replace_label(x) for x in data['annotations'] if x['category_id']==0 or x['category_id']==3]\n",
    "# Only keep images which have these annotations\n",
    "images_to_keep = list(set([x['image_id'] for x in new_annotations]))\n",
    "new_images = [im for im in data['images'] if im['id'] in images_to_keep]\n",
    "# One big new category : Illustration (instead of 7)\n",
    "new_categories = [{'id' : 0, 'name': 'Illustration', 'supercategory' : 'Content'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "471c4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new dataset\n",
    "new_data = data.copy()\n",
    "new_data['categories'] = new_categories\n",
    "new_data['annotations'] = new_annotations\n",
    "new_data['images'] = new_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbf8a0",
   "metadata": {},
   "source": [
    "Now we want to merge the small batches we annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d55cabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our 4 batches of annotations\n",
    "f = open('data/batch1.json')\n",
    "data1 = json.load(f)\n",
    "\n",
    "f = open('data/batch2.json')\n",
    "data2 = json.load(f)\n",
    "\n",
    "f = open('data/batch3.json')\n",
    "data3 = json.load(f)\n",
    "\n",
    "f = open('data/batch4.json')\n",
    "data4 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "67b7158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace the id of the image\n",
    "def replace_id_images(x, im_offset):\n",
    "    try: \n",
    "        x['id'] = x['id'] + im_offset\n",
    "        return x\n",
    "    except:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c9941149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace the ids in the annotation part of the json\n",
    "def replace_ids_annotations(x, im_offset, an_offset):\n",
    "    try: \n",
    "        x['id'] = x['id'] + an_offset\n",
    "        x['image_id'] = x['image_id'] + im_offset \n",
    "        return x\n",
    "    except:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b2d0794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge two COCO files, changing overlapping indices\n",
    "def merge_two_files(f1, f2):\n",
    "    # Ids of last images and annotations of first file\n",
    "    last_id_image = max([im['id'] for im in f1['images']])\n",
    "    last_id_annotation = max([an['id'] for an in f1['annotations']])\n",
    "    \n",
    "    # Merge images with new indices\n",
    "    new_DFKV_imgs = [replace_id_images(im, last_id_image + 1) for im in f2['images']]\n",
    "    new_DFKV_imgs = [*f1['images'], *new_DFKV_imgs]\n",
    "    \n",
    "    # Merge annotations with new indices\n",
    "    new_DFKV_annos = [replace_ids_annotations(im, last_id_image + 1, last_id_annotation + 1) for im in f2['annotations']]\n",
    "    new_DFKV_annos = [*f1['annotations'], *new_DFKV_annos]\n",
    "    \n",
    "    # New merge data file\n",
    "    new_data = f1.copy()\n",
    "    new_data['annotations'] = new_DFKV_annos\n",
    "    new_data['images'] = new_DFKV_imgs\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5e50bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge them all four into 1 json file\n",
    "r1 = merge_two_files(data1, data2)\n",
    "r2 = merge_two_files(r1, data3)\n",
    "r3 = merge_two_files(r2, data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9a186de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with big newspapers file\n",
    "data_final = merge_two_files(new_data, r3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bd58d",
   "metadata": {},
   "source": [
    "Now that we have all annotations, we just modify a little bit the informations about the dataset, and again make sure that all the annotation labels are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c2ac8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all labels to 0\n",
    "data_final['annotations'] = [replace_label(x) for x in data_final['annotations']]\n",
    "# Change info\n",
    "data_final['info']['description'] = 'Modified Beyond Words Dataset (verified) + DFKV'\n",
    "data_final['info']['URL'] = 'https://github.com/dfk-paris/DFKV-illustrations/tree/main/3_illustration_detection'\n",
    "data_final['info']['year'] = 2022\n",
    "data_final['info']['contributor'] = 'LC Labs + DFK Paris'\n",
    "data_final['info']['date_created'] = '15-03-2022'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff260a",
   "metadata": {},
   "source": [
    "We save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5e46eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(data_final)\n",
    "with open('data/anno_complete.json', 'w') as outfile:\n",
    "    outfile.write(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34325767",
   "metadata": {},
   "source": [
    "And split it into a 80%-20% cut for the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bb4bc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "imgs_ids = [im['id'] for im in data_final['images']]\n",
    "train_ids = random.sample(imgs_ids, k=round(len(imgs_ids) * 0.8))\n",
    "train_imgs = [im for im in data_final['images'] if im['id'] in train_ids]\n",
    "test_imgs = [im for im in data_final['images'] if im['id'] not in train_ids]\n",
    "train_annos = [an for an in data_final['annotations'] if an['image_id'] in train_ids]\n",
    "test_annos = [an for an in data_final['annotations'] if an['image_id'] not in train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0372da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_final.copy()\n",
    "train_data['images'] = train_imgs\n",
    "train_data['annotations'] = train_annos\n",
    "test_data = data_final.copy()\n",
    "test_data['images'] = test_imgs\n",
    "test_data['annotations'] = test_annos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2b05b",
   "metadata": {},
   "source": [
    "We finally save the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e3f58509",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(train_data)\n",
    "with open('data/train_annos.json', 'w') as outfile:\n",
    "    outfile.write(json_string)\n",
    "    \n",
    "json_string = json.dumps(test_data)\n",
    "with open('data/test_annos.json', 'w') as outfile:\n",
    "    outfile.write(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
