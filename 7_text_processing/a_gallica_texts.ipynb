{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0449c8",
   "metadata": {},
   "source": [
    "In this notebook, we use the [document API](https://api.bnf.fr/fr/api-document-de-gallica#scroll-nav__10__2) from Gallica to get the text that are surrounding our illustrations. To do this, we will first need to find back the Gallica IDs from the documents that have at least one illustration, and then we will query their API and convert the documents into the needed format for later topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552986a",
   "metadata": {},
   "source": [
    "### Retrieving the Gallica documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5907caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as de_stop\n",
    "from collections import Counter\n",
    "import json\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b645a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file, the same used for Vikus Viewer\n",
    "data = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0a25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to query the Gallica API\n",
    "# And parse the response to only get the text\n",
    "def from_soup_to_string(request_url):\n",
    "    response = requests.get(request_url) # Request API\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') # Parse response\n",
    "    soup_str = soup.prettify()\n",
    "    tmp = soup_str[soup_str.index(\"<hr/>\") + 5 :] # The text is between <hr/> tags\n",
    "    if \"<hr/>\" not in tmp:\n",
    "        return \"\"\n",
    "    html_text = tmp[:tmp.index(\"<hr/>\")]\n",
    "    p_texts = BeautifulSoup(html_text, 'html.parser').find_all('p') # Extract and concatenate all the texts in <p> divs\n",
    "    whole_text = \"\"\n",
    "    for x in p_texts:\n",
    "        whole_text += x.text.replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
    "    return whole_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb337e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3499it [11:16,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Select Gallica subset\n",
    "data_gallica = data[data[\"_iiif-link\"].str.contains(\"gallica\")]\n",
    "dict_text = dict()\n",
    "dict_text_per_page = dict()\n",
    "for doc in tqdm(data_gallica.iterrows()):\n",
    "    # Add entry in dictionnary with the data entry ID and page as key, and the text as value\n",
    "    dict_text_per_page[doc[1][\"id\"].split(\"_\")[1] + \"_\" + doc[1][\"id\"].split(\"_\")[2]] = from_soup_to_string(doc[1][\"_iiif-link\"] + \".texteBrut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf9ba357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are multiple pages in the same data entry, concatenate them all \n",
    "for k, v in dict_text_per_page.items():\n",
    "    dict_text[k.split(\"_\")[0]] = dict_text.get(k.split(\"_\")[0], \"\") + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e476828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>_description</th>\n",
       "      <th>_artist</th>\n",
       "      <th>_source</th>\n",
       "      <th>_material</th>\n",
       "      <th>_dimensions</th>\n",
       "      <th>_journal-id</th>\n",
       "      <th>_date-artwork</th>\n",
       "      <th>year</th>\n",
       "      <th>keywords</th>\n",
       "      <th>_link-dfkv</th>\n",
       "      <th>_iiif-link</th>\n",
       "      <th>_journal-name</th>\n",
       "      <th>_link-dfkv-md</th>\n",
       "      <th>_link-iiif-md</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9287</th>\n",
       "      <td>ILLU_16301_167_0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1949</td>\n",
       "      <td>Reproduction, Large Illustration</td>\n",
       "      <td>https://dfkv.dfkg.org/ng/index.html#/records/1...</td>\n",
       "      <td>https://gallica.bnf.fr/ark:/12148/bpt6k4226334...</td>\n",
       "      <td>Cahiers d'art</td>\n",
       "      <td>[HERE](https://dfkv.dfkg.org/ng/index.html#/re...</td>\n",
       "      <td>[HERE](https://gallica.bnf.fr/ark:/12148/bpt6k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id _description _artist _source _material _dimensions  \\\n",
       "9287  ILLU_16301_167_0          NaN     NaN     NaN       NaN         NaN   \n",
       "\n",
       "      _journal-id  _date-artwork  year                          keywords  \\\n",
       "9287         1302            NaN  1949  Reproduction, Large Illustration   \n",
       "\n",
       "                                             _link-dfkv  \\\n",
       "9287  https://dfkv.dfkg.org/ng/index.html#/records/1...   \n",
       "\n",
       "                                             _iiif-link  _journal-name  \\\n",
       "9287  https://gallica.bnf.fr/ark:/12148/bpt6k4226334...  Cahiers d'art   \n",
       "\n",
       "                                          _link-dfkv-md  \\\n",
       "9287  [HERE](https://dfkv.dfkg.org/ng/index.html#/re...   \n",
       "\n",
       "                                          _link-iiif-md  \n",
       "9287  [HERE](https://gallica.bnf.fr/ark:/12148/bpt6k...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gallica.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76c87401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save texts to json\n",
    "with open('data/complete_texts_gallica.json', 'w') as fp:\n",
    "    json.dump(dict_text, fp)\n",
    "with open('data/complete_texts_page_gallica.json', 'w') as fp:\n",
    "    json.dump(dict_text_per_page, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2b05e",
   "metadata": {},
   "source": [
    "## Create Bag-of-Word files\n",
    "\n",
    "Now that we have all the texts that are around the illustrations, we can prepare the data for further use. We first clean the texts by removing special characters, that we list bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5af538ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/complete_texts_gallica.json\", \"r\") as fp:\n",
    "    dict_text = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2474dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:03<00:00, 123.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Classify between French and German texts\n",
    "german = dict()\n",
    "french = dict()\n",
    "\n",
    "for k, t in tqdm(data.items()):\n",
    "    language = langid.classify(t.replace(\"\\n\", \" \"))\n",
    "    if language[0] == \"de\":\n",
    "        german[k] = t.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        french[k] = t.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b627a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\"\\n\", \"'\", \",\", \";\", \":\", \".\", \"!\", \"?\", \"’\", \"(\", \")\", \"\\\"\", \"%\", \"#\", \"$\", \"&\", \"*\", \"+\", '-', \"[\", \">\", \"]\", \"_\", \"”\", \"“\", \"=\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a71bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    # Remove the special characters\n",
    "    for p in to_remove:\n",
    "        doc = doc.replace(p, \" \")\n",
    "    # Put everything in lowercase\n",
    "    tokens = doc.lower().split(\" \")\n",
    "    # Exclde stopwords and words that have a digit or weird character in them\n",
    "    tokens = [t for t in tokens if t not in fr_stop and t not in de_stop and t != '' and len(t)>2 and \"^\" not in t and \"•\" not in t and \"<\" not in t and \"/\" not in t and not any(map(str.isdigit, t))]\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39292f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each text\n",
    "dict_token_docs = dict()\n",
    "for k, v in dict_text.items():\n",
    "    dict_token_docs[k] = tokenize(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc93472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab = set()\n",
    "for counter in dict_token_docs.values():\n",
    "    vocab = vocab.union(set(counter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e913c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabetically sort the vocabulary\n",
    "vocab = sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b72034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vocab list that we will use to store the actual words written in the vocab document\n",
    "new_vocab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46c17dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary document\n",
    "# Contains the list of words\n",
    "# One word per line\n",
    "with open('data/vocab.dfkv.txt', 'a') as f:\n",
    "    f.truncate(0)\n",
    "    for v in vocab:\n",
    "        try:\n",
    "            f.writelines(str(v) + \"\\n\")\n",
    "            new_vocab.append(v)\n",
    "        except:\n",
    "            vocab.remove(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0260bc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [02:11<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the docword document\n",
    "with open('data/docword.dfkv.txt', 'a') as f:\n",
    "    f.truncate(0)\n",
    "    total_tokens = sum([sum(list(c.values())) for c in dict_token_docs.values()])\n",
    "    # Header lines :\n",
    "    # Number of documents\n",
    "    # Number of unique words\n",
    "    # Number of words in total\n",
    "    f.writelines([str(len(dict_token_docs.keys())) + \"\\n\",  str(len(new_vocab))+ \"\\n\", str(total_tokens) + \"\\n\"])\n",
    "    for k, v in tqdm(sorted(dict_token_docs.items())):\n",
    "        try:\n",
    "            for w, n in v.items():\n",
    "                # Each line consists of :\n",
    "                # DOC_ID WORD_ID WORD_COUNT\n",
    "                to_write = str(k) + \" \" + str((new_vocab.index(w) + 1)) + \" \" + str(n) + \"\\n\"\n",
    "                f.writelines(to_write)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92880c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
