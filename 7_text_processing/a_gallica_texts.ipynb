{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0449c8",
   "metadata": {},
   "source": [
    "In this notebook, we use the [document API](https://api.bnf.fr/fr/api-document-de-gallica#scroll-nav__10__2) from Gallica to get the text that are surrounding our illustrations. To do this, we will first need to find back the Gallica IDs from the documents that have at least one illustration, and then we will query their API and convert the documents into the needed format for later topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552986a",
   "metadata": {},
   "source": [
    "### Retrieving the Gallica documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5907caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3b645a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file, the same used for Vikus Viewer\n",
    "data = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6e0a25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to query the Gallica API\n",
    "# And parse the response to only get the text\n",
    "def from_soup_to_string(request_url):\n",
    "    response = requests.get(request_url) # Request API\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') # Parse response\n",
    "    soup_str = soup.prettify()\n",
    "    tmp = soup_str[soup_str.index(\"<hr/>\") + 5 :] # The text is between <hr/> tags\n",
    "    if \"<hr/>\" not in tmp:\n",
    "        return \"\"\n",
    "    html_text = tmp[:tmp.index(\"<hr/>\")]\n",
    "    p_texts = BeautifulSoup(html_text, 'html.parser').find_all('p') # Extract and concatenate all the texts in <p> divs\n",
    "    whole_text = \"\"\n",
    "    for x in p_texts:\n",
    "        whole_text += x.text.replace(\"\\n\", \"\").replace(\"\\\\\", \"\")\n",
    "    return whole_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb337e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3499it [11:48,  4.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Select Gallica subset\n",
    "data_gallica = data[data[\"_iiif-link\"].str.contains(\"gallica\")]\n",
    "dict_text = dict()\n",
    "for doc in tqdm(data_gallica.iterrows()):\n",
    "    # Add entry in dictionnary with the data entry ID as key, and the text as value\n",
    "    # If there are multiple pages in the same data entry, concatenate them all \n",
    "    dict_text[doc[1][\"id\"].split(\"_\")[1]] = dict_text.get(doc[1][\"id\"].split(\"_\")[1], \"\") + from_soup_to_string(doc[1][\"_iiif-link\"] + \".texteBrut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5e476828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>_description</th>\n",
       "      <th>_artist</th>\n",
       "      <th>_source</th>\n",
       "      <th>_material</th>\n",
       "      <th>_dimensions</th>\n",
       "      <th>_journal-id</th>\n",
       "      <th>_date-artwork</th>\n",
       "      <th>year</th>\n",
       "      <th>keywords</th>\n",
       "      <th>_link-dfkv</th>\n",
       "      <th>_iiif-link</th>\n",
       "      <th>_journal-name</th>\n",
       "      <th>_link-dfkv-md</th>\n",
       "      <th>_link-iiif-md</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>ILLU_15698_150_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1912</td>\n",
       "      <td>Reproduction, Small Illustration</td>\n",
       "      <td>https://dfkv.dfkg.org/ng/index.html#/records/1...</td>\n",
       "      <td>https://gallica.bnf.fr/ark:/12148/bpt6k6106552...</td>\n",
       "      <td>L'Art et les artistes</td>\n",
       "      <td>[HERE](https://dfkv.dfkg.org/ng/index.html#/re...</td>\n",
       "      <td>[HERE](https://gallica.bnf.fr/ark:/12148/bpt6k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id _description _artist _source _material _dimensions  \\\n",
       "4136  ILLU_15698_150_2          NaN     NaN     NaN       NaN         NaN   \n",
       "\n",
       "      _journal-id  _date-artwork  year                          keywords  \\\n",
       "4136         1479            NaN  1912  Reproduction, Small Illustration   \n",
       "\n",
       "                                             _link-dfkv  \\\n",
       "4136  https://dfkv.dfkg.org/ng/index.html#/records/1...   \n",
       "\n",
       "                                             _iiif-link  \\\n",
       "4136  https://gallica.bnf.fr/ark:/12148/bpt6k6106552...   \n",
       "\n",
       "              _journal-name  \\\n",
       "4136  L'Art et les artistes   \n",
       "\n",
       "                                          _link-dfkv-md  \\\n",
       "4136  [HERE](https://dfkv.dfkg.org/ng/index.html#/re...   \n",
       "\n",
       "                                          _link-iiif-md  \n",
       "4136  [HERE](https://gallica.bnf.fr/ark:/12148/bpt6k...  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gallica.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2b05e",
   "metadata": {},
   "source": [
    "## Create Bag-of-Word files\n",
    "\n",
    "Now that we have all the texts that are around the illustrations, we can prepare the data for further use. We first clean the texts by removing special characters, that we list bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "7b627a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\"'\", \",\", \";\", \":\", \".\", \"!\", \"?\", \"’\", \"(\", \")\", \"\\\"\", \"%\", \"#\", \"$\", \"&\", \"*\", \"+\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "1a71bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    # Remove the special characters\n",
    "    for p in to_remove:\n",
    "        doc = doc.replace(p, \" \")\n",
    "    # Put everything in lowercase\n",
    "    tokens = doc.lower().split(\" \")\n",
    "    # Exclde stopwords and words that have a digit or weird character in them\n",
    "    tokens = [t for t in tokens if t not in fr_stop and t != '' and len(t)>1 and \"^\" not in t and \"<\" not in t and not any(map(str.isdigit, t))]\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "39292f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each text\n",
    "dict_token_docs = dict()\n",
    "for k, v in dict_text.items():\n",
    "    dict_token_docs[k] = tokenize(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "dc93472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab = set()\n",
    "for counter in dict_token_docs.values():\n",
    "    vocab = vocab.union(set(counter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "e913c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabetically sort the vocabulary\n",
    "vocab = sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "9b72034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vocab list that we will use to store the actual words written in the vocab document\n",
    "new_vocab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "46c17dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary document\n",
    "# Contains the list of words\n",
    "# One word per line\n",
    "with open('data/vocab.dfkv.txt', 'a') as f:\n",
    "    f.truncate(0)\n",
    "    for v in vocab:\n",
    "        try:\n",
    "            f.writelines(str(v) + \"\\n\")\n",
    "            new_vocab.append(v)\n",
    "        except:\n",
    "            vocab.remove(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "0260bc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [01:57<00:00,  3.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the docword document\n",
    "with open('data/docword.dfkv.txt', 'a') as f:\n",
    "    f.truncate(0)\n",
    "    total_tokens = sum([sum(list(c.values())) for c in dict_token_docs.values()])\n",
    "    # Header lines :\n",
    "    # Number of documents\n",
    "    # Number of unique words\n",
    "    # Number of words in total\n",
    "    f.writelines([str(len(dict_token_docs.keys())) + \"\\n\",  str(len(new_vocab))+ \"\\n\", str(total_tokens) + \"\\n\"])\n",
    "    for k, v in tqdm(sorted(dict_token_docs.items())):\n",
    "        try:\n",
    "            for w, n in v.items():\n",
    "                # Each line consists of :\n",
    "                # DOC_ID WORD_ID WORD_COUNT\n",
    "                to_write = str(k) + \" \" + str((new_vocab.index(w) + 1)) + \" \" + str(n) + \"\\n\"\n",
    "                f.writelines(to_write)\n",
    "        except:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
