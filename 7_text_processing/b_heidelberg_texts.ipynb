{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa077f4",
   "metadata": {},
   "source": [
    "In this notebook, we will use Google Vision API to extract text from the images from the Heidelberg Collection in our DFKV Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82c357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import langid\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as de_stop\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "# Importing Image class from PIL module\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700eca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable with your own secret key\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"absolute/path/to/your/key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5290aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(url):\n",
    "    # Opens a image in RGB mode\n",
    "    im = Image.open(url)\n",
    "\n",
    "    # Size of the image in pixels (size of original image)\n",
    "    width, height = im.size\n",
    "\n",
    "    # Cropped image of above dimension\n",
    "    # (It will not change original image)\n",
    "    im1 = im.crop((0, 0, width, 19 * height / 20))\n",
    "\n",
    "    # Shows the image in image viewer\n",
    "    im1.save(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "840640a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "heidelberg_pages = sorted(glob.glob(\"./data/heidelberg_data/*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58198abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5122/5122 [09:02<00:00,  9.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# crop from all the images the bottom, which contains its url\n",
    "for page in tqdm(heidelberg_pages):\n",
    "    resize_image(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9f88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    # Read the image\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    \n",
    "    # OCR\n",
    "    image = vision.Image(content=content)\n",
    "    response = client.document_text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "    #return doc_id and text\n",
    "    return path.split(\"/\")[3].split(\"_\")[1], path.split(\"/\")[3].split(\"_\")[2].split(\".\")[0], texts[0].description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f154d6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6584/6584 [1:49:29<00:00,  1.00it/s]  \n"
     ]
    }
   ],
   "source": [
    "# OCR all the documents and save them in a dictionnary\n",
    "all_texts_heidelberg = dict()\n",
    "all_texts_pages_heidelberg = dict()\n",
    "for page in tqdm(heidelberg_pages):\n",
    "    doc_id, page, text = detect_text(page)\n",
    "    #all_texts_heidelberg[doc_id] = all_texts_heidelberg.get(doc_id, \"\") + text\n",
    "    all_texts_pages_heidelberg[doc_id + \"_\" + page] = text\n",
    "\n",
    "# If there are multiple pages in the same data entry, concatenate them all \n",
    "for k, v in all_texts_pages_heidelberg.items():\n",
    "    all_texts_heidelberg[k.split(\"_\")[0]] = all_texts_heidelberg.get(k.split(\"_\")[0], \"\") + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db59da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the texts in a json file\n",
    "with open('./data/complete_texts_heidelberg.json', 'w') as fp:\n",
    "    json.dump(all_texts_heidelberg, fp)\n",
    "with open('./data/complete_texts_page_heidelberg.json', 'w') as fp:\n",
    "    json.dump(all_texts_pages_heidelberg, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66139e",
   "metadata": {},
   "source": [
    "## Cleaning and tokenize\n",
    "\n",
    "We will now clean the obtained texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ee0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "with open('./data/complete_texts_heidelberg.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a29a361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:10<00:00, 94.69it/s] \n"
     ]
    }
   ],
   "source": [
    "# Classify between French and German texts\n",
    "german = dict()\n",
    "french = dict()\n",
    "\n",
    "for k, t in tqdm(data.items()):\n",
    "    language = langid.classify(t.replace(\"\\n\", \" \"))\n",
    "    if language[0] == \"fr\":\n",
    "        french[k] = t.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        german[k] = t.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ebf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of charachters to remove from texts\n",
    "to_remove = [\"\\n\", \"-\", \"'\", \",\", \";\", \":\", \".\", \"!\", \"?\", \"’\", \"(\", \")\", \"\\\"\", \"%\", \"#\", \"$\", \"&\", \"*\", \"+\", \"=\",\n",
    "            \"/\", \"…\", \">\", \"@\", \"_\", \"[\", \"]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509ec85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    # Remove the special characters\n",
    "    for p in to_remove:\n",
    "        doc = doc.replace(p, \" \")\n",
    "    # Put everything in lowercase\n",
    "    tokens = doc.lower().split(\" \")\n",
    "    # Exclde stopwords and words that have a digit or weird character in them\n",
    "    tokens = [t for t in tokens if t not in de_stop and t not in fr_stop and t != '' and len(t)>2  and \"http\" not in t and \"^\" not in t and \"<\" not in t and not any(map(str.isdigit, t))]\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1f2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each text\n",
    "dict_token_docs = dict()\n",
    "for k, v in german.items():\n",
    "    dict_token_docs[k] = tokenize(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996ad8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab = set()\n",
    "for counter in dict_token_docs.values():\n",
    "    vocab = vocab.union(set(counter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90160fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabetically sort the vocabulary\n",
    "vocab = sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba5b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vocab list that we will use to store the actual words written in the vocab document\n",
    "new_vocab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "548175b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary document\n",
    "# Contains the list of words\n",
    "# One word per line\n",
    "with open('data/vocab.de_dfkv.txt', 'a') as f:\n",
    "    f.truncate(0)\n",
    "    for v in vocab:\n",
    "        try:\n",
    "            f.writelines(str(v) + \"\\n\")\n",
    "            new_vocab.append(v)\n",
    "        except:\n",
    "            vocab.remove(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80676697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [05:15<00:00,  3.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the docword document\n",
    "with open('data/docword.de_dfkv.txt', 'a') as f:\n",
    "    f.truncate(0)\n",
    "    total_tokens = sum([sum(list(c.values())) for c in dict_token_docs.values()])\n",
    "    # Header lines :\n",
    "    # Number of documents\n",
    "    # Number of unique words\n",
    "    # Number of words in total\n",
    "    f.writelines([str(len(dict_token_docs.keys())) + \"\\n\",  str(len(new_vocab))+ \"\\n\", str(total_tokens) + \"\\n\"])\n",
    "    for k, v in tqdm(sorted(dict_token_docs.items())):\n",
    "        try:\n",
    "            for w, n in v.items():\n",
    "                # Each line consists of :\n",
    "                # DOC_ID WORD_ID WORD_COUNT\n",
    "                to_write = str(k) + \" \" + str((new_vocab.index(w) + 1)) + \" \" + str(n) + \"\\n\"\n",
    "                f.writelines(to_write)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33be982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
